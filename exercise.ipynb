{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check everything is set up and ready\n",
    "import sklearn\n",
    "import numpy\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82218abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('matches.pickle', 'rb') as f:\n",
    "    matches = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75994e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some exploratory analysis here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf313c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [] # the correct labels\n",
    "baseline_predictions = []\n",
    "\n",
    "# write code here...\n",
    "\n",
    "\n",
    "# .. so that the asserts pass\n",
    "assert len(y) == len(baseline_predictions) == len(matches)\n",
    "assert sum(y) == 15739\n",
    "assert sum(baseline_predictions) == 37010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array(y)\n",
    "baseline_predictions = np.array(baseline_predictions)\n",
    "\n",
    "sum(baseline_predictions == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a26629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.pickle', 'rb') as f:\n",
    "  member_embeddings, request_embeddings = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa584d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "for match in matches:\n",
    "    X.append(member_embeddings[match['member_id']] * \\\n",
    "        request_embeddings[match['request_id']])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test, _, baseline_predictions_test = \\\n",
    "    train_test_split(X, y, baseline_predictions, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_v1 = XGBClassifier(\n",
    "    n_estimators=5, # <- if on CPU, reduce this to.. 5?\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    #gpu_id=0, # if you don't have a GPU set up, remove this and run on CPU\n",
    "    #tree_method='gpu_hist', # if on CPU, remove this\n",
    ")\n",
    "model_v1.fit(X_train, y_train)\n",
    "model_v1_predictions = model_v1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline:', sum(baseline_predictions_test == y_test)/len(y_test))\n",
    "print('new model:', sum(model_v1_predictions == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe78d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTEND_COUNT = 120000\n",
    "# list of dictionaries {'member_id': int, 'request_id': int}\n",
    "true_negatives = []\n",
    "\n",
    "# generate 120000 random new matches ...\n",
    "\n",
    "\n",
    "# ... such that these tests pass\n",
    "assert len(true_negatives) == EXTEND_COUNT\n",
    "true_negatives_set = {(tn['member_id'], tn['request_id']) for tn in true_negatives}\n",
    "assert len(true_negatives_set) == EXTEND_COUNT\n",
    "\n",
    "old_matches_set = {(d['member_id'], d['request_id']) for d in matches}\n",
    "member_ids_set = {d['member_id'] for d in matches}\n",
    "request_ids_set = {d['request_id'] for d in matches}\n",
    "\n",
    "assert len(true_negatives_set.difference(old_matches_set)) == EXTEND_COUNT\n",
    "for tn in true_negatives:\n",
    "    assert tn['member_id'] in member_ids_set\n",
    "    assert tn['request_id'] in request_ids_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tn = []\n",
    "y_tn = []\n",
    "baseline_predictions_tn = []\n",
    "\n",
    "# generate a dataset solely for these new true negatives...\n",
    "\n",
    "\n",
    "# .. so that the asserts pass\n",
    "assert len(X_tn) == len(y_tn) == len(baseline_predictions_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split this new dataset into train and test...\n",
    "\n",
    "\n",
    "# ... then extend the previous dataset...\n",
    "\n",
    "\n",
    "# ... so that the asserts pass\n",
    "assert X_train_extended.shape == (151405, 768)\n",
    "assert X_test_extended.shape == (7969, 768)\n",
    "assert len(y_train_extended) == 151405\n",
    "assert len(y_test_extended) == 7969\n",
    "assert len(baseline_predictions_test_extended) == 7969\n",
    "assert sum(y_train_extended) == 14913\n",
    "assert sum(y_test_extended) == 826\n",
    "assert sum(baseline_predictions_test_extended) == 1829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7398d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# use the same model setup as above regarding GPU/CPU\n",
    "model_v2 = XGBClassifier(\n",
    "    n_estimators=5, # <- if on CPU, reduce this to.. 5?\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    #gpu_id=0, # if you don't have a GPU set up, remove this and run on CPU\n",
    "    #tree_method='gpu_hist', # if on CPU, remove this\n",
    ")\n",
    "model_v2.fit(X_train_extended, y_train_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1_predictions = model_v1.predict(X_test_extended)\n",
    "model_v2_predictions = model_v2.predict(X_test_extended)\n",
    "\n",
    "print('baseline:', sum(baseline_predictions_test_extended == y_test_extended)/len(y_test_extended))\n",
    "print('model v1:', sum(model_v1_predictions == y_test_extended)/len(y_test_extended))\n",
    "print('model v2:', sum(model_v2_predictions == y_test_extended)/len(y_test_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b31b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "model_v1_predictions = model_v1.predict_proba(X_test_extended)[:,1]\n",
    "model_v2_predictions = model_v2.predict_proba(X_test_extended)[:,1]\n",
    "\n",
    "print('baseline:', metrics.roc_auc_score(y_test_extended, baseline_predictions_test_extended))\n",
    "print('model v1:', metrics.roc_auc_score(y_test_extended, model_v1_predictions))\n",
    "print('model v2:', metrics.roc_auc_score(y_test_extended, model_v2_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4937df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
